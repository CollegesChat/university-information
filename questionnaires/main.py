#!/usr/bin/env python3
# -*- encoding: utf-8 -*-

from slugify import slugify
import collections
import csv
import os
import re
import time
import shutil
import typing
import zhconv
from datetime import date, datetime
from typing import IO

archive_time = "2023-01-01 00:00:00"  # any questionnaire submitted before this will be archived

questionnaire = [
    'å®¿èˆæ˜¯ä¸ŠåºŠä¸‹æ¡Œå—ï¼Ÿ',
    'æ•™å®¤å’Œå®¿èˆæœ‰æ²¡æœ‰ç©ºè°ƒï¼Ÿ',
    'æœ‰ç‹¬ç«‹å«æµ´å—ï¼Ÿæ²¡æœ‰ç‹¬ç«‹æµ´å®¤çš„è¯ï¼Œæ¾¡å ‚ç¦»å®¿èˆå¤šè¿œï¼Ÿ',
    'æœ‰æ—©è‡ªä¹ ã€æ™šè‡ªä¹ å—ï¼Ÿ',
    'æœ‰æ™¨è·‘å—ï¼Ÿ',
    'æ¯å­¦æœŸè·‘æ­¥æ‰“å¡çš„è¦æ±‚æ˜¯å¤šå°‘å…¬é‡Œï¼Œå¯ä»¥éª‘è½¦å—ï¼Ÿ',
    'å¯’æš‘å‡æ”¾å¤šä¹…ï¼Œæ¯å¹´å°å­¦æœŸæœ‰å¤šé•¿ï¼Ÿ',
    'å­¦æ ¡å…è®¸ç‚¹å¤–å–å—ï¼Œå–å¤–å–çš„åœ°æ–¹ç¦»å®¿èˆæ¥¼å¤šè¿œï¼Ÿ',
    'å­¦æ ¡äº¤é€šä¾¿åˆ©å—ï¼Œæœ‰åœ°é“å—ï¼Œåœ¨å¸‚åŒºå—ï¼Œä¸åœ¨çš„è¯è¿›åŸè¦å¤šä¹…ï¼Ÿ',
    'å®¿èˆæ¥¼æœ‰æ´—è¡£æœºå—ï¼Ÿ',
    'æ ¡å›­ç½‘æ€ä¹ˆæ ·ï¼Ÿ',
    'æ¯å¤©æ–­ç”µæ–­ç½‘å—ï¼Œå‡ ç‚¹å¼€å§‹æ–­ï¼Ÿ',
    'é£Ÿå ‚ä»·æ ¼è´µå—ï¼Œä¼šåƒå‡ºå¼‚ç‰©å—ï¼Ÿ',
    'æ´—æ¾¡çƒ­æ°´ä¾›åº”æ—¶é—´ï¼Ÿ',
    'æ ¡å›­å†…å¯ä»¥éª‘ç”µç“¶è½¦å—ï¼Œç”µæ± åœ¨å“ªèƒ½å……ç”µï¼Ÿ',
    'å®¿èˆé™ç”µæƒ…å†µï¼Ÿ',
    'é€šå®µè‡ªä¹ æœ‰å»å¤„å—ï¼Ÿ',
    'å¤§ä¸€èƒ½å¸¦ç”µè„‘å—ï¼Ÿ',
    'å­¦æ ¡é‡Œé¢ç”¨ä»€ä¹ˆå¡ï¼Œé¥­å ‚æ€æ ·æ¶ˆè´¹ï¼Ÿ',
    'å­¦æ ¡ä¼šç»™å­¦ç”Ÿå‘é“¶è¡Œå¡å—ï¼Ÿ',
    'å­¦æ ¡çš„è¶…å¸‚æ€ä¹ˆæ ·ï¼Ÿ',
    'å­¦æ ¡çš„æ”¶å‘å¿«é€’æ”¿ç­–æ€ä¹ˆæ ·ï¼Ÿ',
    'å­¦æ ¡é‡Œé¢çš„å…±äº«å•è½¦æ•°ç›®ä¸ç§ç±»å¦‚ä½•ï¼Ÿ',
    'ç°é˜¶æ®µå­¦æ ¡çš„é—¨ç¦æƒ…å†µå¦‚ä½•ï¼Ÿ',
    'å®¿èˆæ™šä¸ŠæŸ¥å¯å—ï¼Œå°å¯å—ï¼Œæ™šå½’èƒ½å›å»å—ï¼Ÿ']


NAME_PREPROCESS = re.compile(r'[\(\)ï¼ˆï¼‰ã€ã€‘#]')
FILENAME_PREPROCESS = re.compile(r'[/>|:&]')
NORMAL_NAME_MATCHER = re.compile(r'å¤§å­¦|å­¦é™¢|å­¦æ ¡')


# store answer id and content
class IndexedContent:
    answer_id: int
    content: str

    def __init__(self, answer_id: int, content: str):
        self.answer_id = answer_id
        self.content = content

    def __str__(self):
        # print answer correctly when concatenating with str
        return f'A{self.answer_id}: {self.content}'


class AnswerGroup:
    answers: list

    def __init__(self):
        self.answers = []

    def add_answer(self, answer: IndexedContent):
        self.answers.append(answer)

    def extend(self, other):
        self.answers.extend(other.answers)


class University:
    answers: list
    additional_answers: list
    credits: list

    def __init__(self):
        self.answers = [AnswerGroup() for _ in range(len(questionnaire))]
        self.additional_answers = []
        self.credits = []

    def add_answer(self, index: int, answer: IndexedContent):
        self.answers[index].add_answer(answer)

    def add_additional_answer(self, answer: IndexedContent):
        self.additional_answers.append(answer)

    def add_credit(self, author: IndexedContent):
        self.credits.append(author)

    def combine_from(self, other):
        for this, that in zip(self.answers, other.answers):
            this.extend(that)
        self.additional_answers.extend(other.additional_answers)
        self.credits.extend(other.credits)


class FilenameMap:
    mapping: dict
    already_exists: set

    def __init__(self):
        self.mapping = {}
        self.already_exists = set()

    def format(self, name: str, index: int):
        if index > 1:
            return f'{name}-{index}'
        else:
            return name

    def __getitem__(self, name: str):
        value = self.mapping.get(name)
        if value is None:
            value = slugify(FILENAME_PREPROCESS.sub('', name.replace(' ', '-')))
            index = 1
            while self.format(value, index) in self.already_exists:
                index += 1
            value = self.format(value, index)
            self.mapping[name] = value
            self.already_exists.add(value)
        return value


def markdown_escape(text: str):
    return text.replace('*', '\\*').replace('~', '\\~').replace('_', '\\_')


def join_path(*paths):
    # return os.path.join(*paths)
    return '/'.join(paths)


def generate_markdown_path(name: str, in_readme: bool, archived: bool):
    paths = [ 'universities', name ]
    if archived and not in_readme:
        paths = ['archived'] + paths
    if in_readme and not archived:
        paths = ['.', 'docs'] + paths
    return join_path(*paths) + '.md'


def load_colleges():
    colleges = {}
    provinces = {}
    with open('colleges.csv', 'r', encoding='utf-8') as f:
        csv_reader = csv.reader(f)

        for row in csv_reader:
            province, college = row
            colleges[NAME_PREPROCESS.sub('', college).replace(' ','')] = province
            if province not in provinces:
                provinces[province] = []
        provinces['å…¶ä»–'] = []

    # `provinces`: an dict whose keys are provinces and values are empty
    # `colleges`: dict, college name => province
    return provinces, colleges  


def load_to_universities(universities: dict, row: list):
    # unpack row into different parts, and ignore 9 items in the end.
    # `anonymous`: `2` means anonymous, and `1` not.
    # if `anonymous` is True, `email` is empty.
    aid, _, anonymous, email, show_email, name, *answers = row[:-9]
    # if int(id) == 3516:
    #     continue

    additional_answer = IndexedContent(aid, row[-9])

    # convert `anonymous` and `show_email` to boolean
    anonymous = True if int(anonymous) == 2 else False
    show_email = True if (not anonymous and float(show_email) == 1.0) else False

    # preprocess name
    name = zhconv.convert(name, 'zh-cn')
    name = NAME_PREPROCESS.sub('', name).strip()

    # if not exists, defaultdict will help create one
    university = universities[name]

    # process questionnaire submittal time
    submittal_time = datetime.strptime(row[-8], '%Y-%m-%d %H:%M:%S')

    if not show_email or email == '':
        university.add_credit(IndexedContent(aid, 'åŒ¿å (' + submittal_time.strftime('%Y å¹´ %m æœˆ') + ')'))
    else:
        university.add_credit(IndexedContent(aid, email + ' (' + submittal_time.strftime('%Y å¹´ %m æœˆ') + ')'))

    for index, answer in enumerate(answers):
        university.add_answer(index, IndexedContent(aid, answer))
    university.add_additional_answer(additional_answer)


def process_universities(universities: dict, colleges: dict):
    # ===== combine colleges =====
    with open('alias.txt', 'r', encoding='utf-8') as f:
        for line in f:
            name, *aliases = line.rstrip('\n').split('ğŸš®')
            university = universities[name]
            for alias in aliases:
                university.combine_from(universities[alias])
                del universities[alias]
            if len(university.credits) == 0:
                del universities[name]

    with open('blacklist.txt', 'r', encoding='utf-8') as f:
        for line in f:
            name = line.rstrip('\n')
            if name in universities:
                del universities[name]
                
    middle_school_names = []
    for name in universities:
        if (name.endswith('ä¸­') or 'ä¸­å­¦' in name or name.endswith('é«˜') or name.endswith('å®éªŒ')) and name not in colleges:
            middle_school_names.append(name)
    for name in middle_school_names:
        print(f'[info] \033[0;36m{name}\033[0m is removed')
        del universities[name]
    
    whitelist = set()
    with open('whitelist.txt', 'r', encoding='utf-8') as f:
        for line in f:
            tmp_name = line.rstrip('\n')
            whitelist.add(tmp_name)
            
    for name in universities.keys():
        if NORMAL_NAME_MATCHER.search(name) is None:
            if not name in whitelist:
                print(f'[warning] \033[0;36m{name}\033[0m may be invalid')


def write_to_markdown(universities: dict, filename_map: FilenameMap, archived: bool):
    for name, university in universities.items():
        filename = generate_markdown_path(filename_map[name], False, archived)
        if archived:
            name += ' (å·²å½’æ¡£)'
        folder_name = join_path('dist', 'docs')

        # Ignored samples with excessively long names
        if len(filename) > 150:
            continue

        with open(join_path(folder_name, filename), 'w', encoding='utf-8') as f:
            # write header
            f.write(f'# {name}\n\n')
            f.write('> [å…è´£å£°æ˜](https://colleges.chat/#_3)ï¼šæœ¬é¡µé¢å†…å®¹å‡æ¥æºäºé—®å·æ”¶é›†ï¼Œä»…ä¾›å‚è€ƒï¼Œè¯·è‡ªè¡Œç¡®å®šä¿¡æ¯å‡†ç¡®æ€§å’ŒçœŸå®æ€§ï¼\n\n')
            f.write('> è‹¥æ‚¨å‘ç°å›ç­”ä¸­å­˜åœ¨ç­”éæ‰€é—®æˆ–èƒ¡è¨€ä¹±è¯­ï¼Œæ¬¢è¿è®°å½•å¯¹åº”çš„é—®å· IDï¼Œå‰å¾€é¡µé¢å¯¹åº”çš„ GitHub é¡µé¢ï¼Œé€šè¿‡ issue æˆ–é‚®ä»¶ç­‰æ–¹å¼æäº¤åé¦ˆï¼\n\n')
            output_credits = '> æ•°æ®æ¥æºï¼š\n\n<details><summary>ç‚¹å‡»å±•å¼€</summary>\n<ul>\n'
            for credit in university.credits:
                output_credits += f'<li>A{credit.answer_id}: {credit.content}</li>\n'
            f.write(output_credits + '</ul>\n</details>\n\n')

            # write answers
            assert len(questionnaire) == len(university.answers)
            for question, answers in zip(questionnaire, university.answers):
                f.write(f'## Q: {question}\n\n')
                for answer in answers.answers:
                    f.write(f'- A{answer.answer_id}: {markdown_escape(answer.content)}\n\n')

            # write additional answers
            additional_answers = [markdown_escape(answer.__str__()).replace('\n', '\n\n')
                                   for answer in university.additional_answers if len(answer.content) > 0]
            if len(additional_answers) > 0:
                f.write('## è‡ªç”±è¡¥å……éƒ¨åˆ†\n\n')
                f.write('\n\n***\n\n'.join(additional_answers))


def write_to_readme(universities: dict, filename_map: FilenameMap, readme_file_name: str, readme_template_name: str, nav_file_name: str, provinces: dict, colleges: dict, archived=False):  
    with open(readme_file_name, 'w', encoding='utf-8') as readme_file,\
         open(readme_template_name, 'r', encoding='utf-8') as template_file,\
         open(nav_file_name, 'w', encoding='utf-8') as nav_file:

        # first, copy template
        template = template_file.read()
        readme_file.write(template)
        readme_file.write('\n\n')

        # write university links
        suffix = ''
        if archived:
            suffix = ' (å·²å½’æ¡£)'
        university_names = list(universities.keys())
        university_names.sort()
        # here `in_readme` should be opposite from `archived` to avoid generating redundant 'docs' for archived
        university_links = [ '[{}]({})'.format(name + suffix, generate_markdown_path(filename_map[name], not archived, False)) for name in university_names ]
        readme_file.write('\n\n'.join(university_links))

        sorted_colleges_keys = sorted(colleges.keys())

        for name in university_names:
            belong_province = 'å…¶ä»–'
            last_pos = 114514
            for college in sorted_colleges_keys:
                current_pos = name.find(college)
                if (current_pos >= 0) and (current_pos <= last_pos):
                    belong_province = colleges[college]
                    last_pos = current_pos
            provinces[belong_province].append(name)

        # and, write renamed colleges
        readme_file.write('\n\n### æ›´åçš„å¤§å­¦\n\n')
        with open('history.txt', 'r', encoding='utf-8') as f:
            for history in f:
                name, *originals = history.rstrip('\n').split('â¬…')
                for original in originals:
                    readme_file.write('{} â†’ [{}]({})\n\n'.format(original, name, generate_markdown_path(filename_map[name], True, archived)))

        for province, college in provinces.items():
            nav_file.write(f'    - {province}:\n')
            college.sort()
            for name in college:
                nav_file.write('      - {}: {}\n'.format(name + suffix, generate_markdown_path(filename_map[name], False, archived)))


def main():
    provinces, colleges = load_colleges()
    provinces_archived, colleges_archived = load_colleges()
    
    archive_date = datetime.strptime(archive_time, '%Y-%m-%d %H:%M:%S')

    # ===== read from csv =====
    with open('results_desensitized.csv', 'r', encoding='gb18030') as f:
        csv_reader = csv.reader(f)
        next(csv_reader)  # here we skip the first line

        universities = collections.defaultdict(University)
        universities_archived = collections.defaultdict(University)
        filename_map = FilenameMap()  # university name => filename
        filename_map_archived = FilenameMap()

        for row in csv_reader:
            submittal_time = datetime.strptime(row[-8], '%Y-%m-%d %H:%M:%S')
            # if `submittal_time` is before `archive_date`, save it to archive dict
            if submittal_time < archive_date:
                load_to_universities(universities_archived, row)
            else:
                load_to_universities(universities, row)

    process_universities(universities, colleges)
    process_universities(universities_archived, colleges)

    # ===== write results =====
    if os.path.exists(join_path('dist', '.git')):
        shutil.move(join_path('dist', '.git'), 'dist.git')
    shutil.rmtree('dist', ignore_errors=True)
    shutil.copytree('site', 'dist')
    if os.path.exists('dist.git'):
        shutil.move('dist.git', join_path('dist', '.git'))
    os.makedirs(join_path('dist', 'docs', 'universities'), exist_ok=True)
    os.makedirs(join_path('dist', 'docs', 'archived', 'universities'), exist_ok=True)

    write_to_markdown(universities, filename_map, False)
    write_to_markdown(universities_archived, filename_map_archived, True)

    # write README.md and nav file
    write_to_readme(universities, filename_map, join_path('dist', 'README.md'), 'README_template.md', join_path('dist', 'nav.txt'), provinces, colleges)
    write_to_readme(universities_archived, filename_map_archived, join_path('dist', 'docs', 'archived', 'README.md'), 'README_archived_template.md', join_path('dist', 'docs', 'archived', 'nav.txt'), provinces_archived, colleges_archived, archived=True)

    # TODO: add archive to mkdocs
    with open(join_path('dist', 'nav.txt'), 'r', encoding='utf-8') as nav_f,\
         open(join_path('dist', 'docs', 'archived', 'nav.txt'), 'r', encoding='utf-8') as nav_archived_f,\
         open('mkdocs_template.yml', 'r', encoding='utf-8') as mkdocs_template_f,\
         open(join_path('dist', 'mkdocs.yml'), 'w', encoding='utf-8') as mkdocs_f:
        
        mkdocs_f.write(mkdocs_template_f.read().replace('[universities_nav]',nav_f.read()).replace('[universities_nav_archived]',nav_archived_f.read()).replace('[current_time]',time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())))

if __name__ == '__main__':
    main()
